{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9ac015-e604-4c60-a9e0-5b4be059589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file contains all the code needed to run the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "import timm\n",
    "from typing import List, Optional\n",
    "from timm.layers import SelectAdaptivePool2d, create_conv2d, GELUTanh\n",
    "import itertools\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7cd4bf-6d28-4214-8c38-c4fb3bc60b4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtimm\u001B[49m\u001B[38;5;241m.\u001B[39mcreate_model(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresnet50\u001B[39m\u001B[38;5;124m'\u001B[39m, pretrained\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(class_labels))\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(model)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'timm' is not defined"
     ]
    }
   ],
   "source": [
    "# deit3_small_patch16_224\n",
    "\n",
    "# swin_base_patch4_window7_224\n",
    "\n",
    "# model = timm.create_model('resnet50', pretrained=True, num_classes=len(class_labels))\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6a593f-5ff4-4bc0-9a26-5ce4a10cf87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_padding(kernel_size ) :\n",
    "    if isinstance(kernel_size , tuple):\n",
    "        return tuple([same_padding(s) for s in kernel_size])\n",
    "    else :\n",
    "        assert kernel_size % 2 != 0, \"kernel size must me odd to make the padding equal across the image\" #if padding is even we might need to add 2 differnt paddings across the width and bredth \n",
    "        return kernel_size // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6963d875-4ecf-4d07-abdd-94089da5a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2tuple(x: list or tuple or any, min_len: int = 1, idx_repeat: int = -1) :\n",
    "\n",
    "    if isinstance(x, tuple):\n",
    "       \n",
    "        return x\n",
    "\n",
    "    \n",
    "    x = val2list(x)\n",
    "\n",
    "    # repeat elements if necessary\n",
    "    if len(x) > 0:\n",
    "        x[idx_repeat:idx_repeat] = [x[idx_repeat] for _ in range(min_len - len(x))]\n",
    "\n",
    "    return tuple(x)\n",
    "\n",
    "\n",
    "def val2list(x: list or tuple or any, repeat_time=1) -> list:\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    return [x for _ in range(repeat_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f457526-070d-4a40-91c4-57ff51b0421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "    in_channels : int,\n",
    "    out_channels : int,\n",
    "    kernel_size  = 3,\n",
    "    stride = 1,\n",
    "    dilation = 1,\n",
    "    groups = 1,\n",
    "    use_bias = True,\n",
    "    dropout = 0 ,\n",
    "    pad = None,\n",
    "    norm  = None,\n",
    "    activation = None,\n",
    "    ):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        pad = same_padding(kernel_size) if pad == None else pad\n",
    "        pad *= dilation\n",
    "        self.dropout = nn.Dropout2d(dropout, inplace=False) \n",
    "\n",
    "\n",
    "       \n",
    "        # print(\"hello\" , in_channels , out_channels , kernel_size , stride , pad , dilation , groups , use_bias)\n",
    "\n",
    "        # Parameter(torch.empty((out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "\n",
    "        \n",
    "        self.Convlayer = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size= (kernel_size,kernel_size),\n",
    "            stride=(stride, stride),\n",
    "            padding=pad,\n",
    "            dilation=(dilation, dilation),\n",
    "            groups=groups,\n",
    "            bias=use_bias,\n",
    "        )\n",
    "        # print(\"BYE\")\n",
    "\n",
    "        if norm == nn.LayerNorm :\n",
    "            self.norm = nn.LayerNorm([8, 256, 14, 14])\n",
    "        else:\n",
    "        \n",
    "            self.norm = norm(num_features=out_channels) if norm else nn.Identity()\n",
    "        \n",
    "        self.activation = activation(inplace = False) if activation is not None else nn.Identity()\n",
    "       \n",
    "        # normalization done last self.norm = \n",
    "        #activation done last \n",
    "\n",
    "    def forward(self , x):\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.Convlayer(x)\n",
    "   \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        x = self.activation(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948f3126-fa50-45ac-82a3-2fba6e2fab48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[-1., -1.],\n",
       "         [ 1.,  1.]],\n",
       "\n",
       "        [[-1., -1.],\n",
       "         [ 1.,  1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[[1,2] , [1,2]] , [[1,2] , [3,4]] , [[0,1] , [2,3]]])\n",
    "\n",
    "p = torch.mean(x, dim=1, keepdim=True)\n",
    "x - p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a917ca4e-86e1-42d4-b68b-191e192aa3df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m parameters_dict_DWConv \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbias\u001B[39m\u001B[38;5;124m'\u001B[39m : (\u001B[38;5;28;01mTrue\u001B[39;00m , \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnorm\u001B[39m\u001B[38;5;124m'\u001B[39m : (\u001B[38;5;28;01mNone\u001B[39;00m , \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mactivation\u001B[39m\u001B[38;5;124m'\u001B[39m : (\u001B[38;5;28;01mNone\u001B[39;00m , \u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mReLU)\n\u001B[1;32m      5\u001B[0m }\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDepthWiseSeperableConvLayer\u001B[39;00m(nn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     10\u001B[0m     in_channels,\n\u001B[1;32m     11\u001B[0m     out_channels,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m     norm  \u001B[38;5;241m=\u001B[39m parameters_dict_DWConv[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnorm\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     16\u001B[0m     activation  \u001B[38;5;241m=\u001B[39m parameters_dict_DWConv[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mactivation\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "parameters_dict_DWConv = {\n",
    "    'bias' : (True , True),\n",
    "    'norm' : (None , None),\n",
    "    'activation' : (None , nn.ReLU)\n",
    "}\n",
    "\n",
    "\n",
    "class DepthWiseSeperableConvLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    kernel_size  = 3,\n",
    "    stride = 1,\n",
    "    use_bias  = parameters_dict_DWConv['bias'],\n",
    "    norm  = parameters_dict_DWConv['norm'],\n",
    "    activation  = parameters_dict_DWConv['activation']):\n",
    "\n",
    "        \n",
    "\n",
    "        super(DepthWiseSeperableConvLayer, self).__init__()\n",
    "\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "        norm = val2tuple(norm, 2)\n",
    "        activation = val2tuple(activation, 2)\n",
    "\n",
    "        self.Splitting = ConvLayer(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size= kernel_size,\n",
    "            stride= stride,\n",
    "            groups=in_channels,\n",
    "            norm = norm[0],\n",
    "            activation = activation[0],\n",
    "            use_bias=use_bias[0],\n",
    "        )\n",
    "        self.Combining = ConvLayer(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=stride,\n",
    "            groups=1,\n",
    "            norm = norm[1],\n",
    "            activation = activation[1],\n",
    "            use_bias=use_bias[1],\n",
    "        )\n",
    "    def forward(self , x : torch.Tensor):\n",
    "        x = self.Splitting(x)\n",
    "        return self.Combining(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2be14518-9bc1-4702-855a-f113e95e3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict_MBConv = {\n",
    "    'bias' : (True , True , False),\n",
    "    'norm' : (None , None , nn.BatchNorm2d),\n",
    "    'activation' : (nn.ReLU6 , nn.ReLU6 , None)\n",
    "}\n",
    "\n",
    "class MobileBottleneckConvLayer(nn.Module):\n",
    "    def __init__(self , \n",
    "    in_channels : int,\n",
    "    out_channels : int,\n",
    "    stride = 1,\n",
    "    kernel_size = 3,\n",
    "    groups = 1,\n",
    "    expand_ratio = 5,\n",
    "    use_bias  = parameters_dict_MBConv['bias'],\n",
    "    norm  = parameters_dict_MBConv['norm'],\n",
    "    activation  = parameters_dict_MBConv['activation']):\n",
    "\n",
    "        super(MobileBottleneckConvLayer, self).__init__()\n",
    "\n",
    "        use_bias = val2tuple(use_bias, 3)\n",
    "        norm = val2tuple(norm, 3)\n",
    "        activation = val2tuple(activation, 3)\n",
    "\n",
    "        \n",
    "\n",
    "        mid_channel = round(in_channels*expand_ratio)\n",
    "        self.Expand = ConvLayer(\n",
    "                in_channels,\n",
    "                mid_channel,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                norm = norm[0],\n",
    "                activation = activation[0],\n",
    "                use_bias=use_bias[0],\n",
    "                groups = groups\n",
    "            \n",
    "                \n",
    "            )\n",
    "        self.Splitting = ConvLayer(\n",
    "                mid_channel,\n",
    "                mid_channel,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                groups=mid_channel,\n",
    "                norm = norm[1],\n",
    "                activation = activation[1],\n",
    "                use_bias=use_bias[1]\n",
    "                \n",
    "            )\n",
    "\n",
    "        \n",
    "        self.Project = ConvLayer(\n",
    "                mid_channel,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                groups = groups,\n",
    "                norm = norm[2],\n",
    "                activation = activation[2],\n",
    "                use_bias=use_bias[2]\n",
    "            )\n",
    "\n",
    "    def forward(self , x : torch.Tensor):\n",
    "        x = self.Expand(x)\n",
    "        x = self.Splitting(x)\n",
    "        return self.Project(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aa8a721-506b-4269-a34e-50a980197e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict_FMBConv = {\n",
    "    'bias' : (True , True),\n",
    "    'norm' : (nn.BatchNorm2d , nn.BatchNorm2d ),\n",
    "    'activation' : (nn.ReLU , nn.ReLU )\n",
    "}\n",
    "\n",
    "class FusedMobileBottleneckConvLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        mid_channels=None,\n",
    "        expand_ratio=6,\n",
    "        groups=1,\n",
    "        use_bias=False,\n",
    "        norm=(nn.BatchNorm2d, nn.BatchNorm2d),\n",
    "        activation=('ReLU6', None),\n",
    "    ):\n",
    "        super(FusedMobileBottleneckConvLayer, self).__init__()\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "\n",
    "        \n",
    "        norm = val2tuple(norm, 2)\n",
    "\n",
    "        if not isinstance(x, tuple):\n",
    "            \n",
    "            activation = val2tuple(activation, 2)\n",
    "            \n",
    "        mid_channels = mid_channels or round(in_channels * expand_ratio)\n",
    "\n",
    "       \n",
    "\n",
    "        self.spatial_conv = ConvLayer(\n",
    "            in_channels,\n",
    "            mid_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            groups=groups,\n",
    "            norm=norm[0],\n",
    "            activation=activation[0],\n",
    "            use_bias=use_bias[0],\n",
    "        )\n",
    "        self.point_conv = ConvLayer(\n",
    "            mid_channels,\n",
    "            out_channels,\n",
    "            1,\n",
    "            norm=norm[1],\n",
    "            activation=activation[1],\n",
    "            use_bias=use_bias[1],\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.point_conv(x)\n",
    "        return x\n",
    "\n",
    "# class FusedMobileBottleneckConvLayer(nn.Module):\n",
    "#     def __init__(self , \n",
    "#     in_channels : int,\n",
    "#     out_channels : int,\n",
    "#     stride = 1,\n",
    "#     kernel_size = 3,\n",
    "#     groups = 1,\n",
    "#     expand_ratio = 5,\n",
    "#     use_bias : tuple[bool,...] = parameters_dict_FMBConv['bias'],\n",
    "#     norm : tuple[str,...] = parameters_dict_FMBConv['norm'],\n",
    "#     activation : tuple[str,...] = parameters_dict_FMBConv['activation']):\n",
    "        \n",
    "#         super(FusedMobileBottleneckConvLayer, self).__init__()\n",
    "\n",
    "#         use_bias = val2tuple(use_bias, 2)\n",
    "#         norm = val2tuple(norm, 2)\n",
    "#         activation = val2tuple(activation, 2)\n",
    "        \n",
    "#         mid_channel = round(in_channels*expand_ratio)\n",
    "\n",
    "\n",
    "#         print(norm,activation,use_bias)\n",
    "#         self.Expand_Depth = ConvLayer(\n",
    "#                 in_channels,\n",
    "#                 mid_channel,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 stride=stride,\n",
    "#                 groups = groups,\n",
    "#                 norm = norm[0],\n",
    "#                 activation = activation[0],\n",
    "#                 use_bias=use_bias[0],\n",
    "#             )\n",
    "        \n",
    "\n",
    "#         self.Project = ConvLayer(\n",
    "#                 mid_channel,\n",
    "#                 out_channels,\n",
    "#                 kernel_size=1,\n",
    "#                 stride=(stride, stride),\n",
    "#                 groups = groups,\n",
    "#                 norm = norm[1],\n",
    "#                 activation = activation[1],\n",
    "#                 use_bias=use_bias[1]\n",
    "#             )\n",
    "\n",
    "#     def forward(self , x) :\n",
    "#         x = self.Expand_Depth(x)\n",
    "#         print(x.shape )\n",
    "#         x = self.Project(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4201bd48-04cd-4bc0-bdb5-f936b794bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict_ResB = {\n",
    "    'bias' : (True , True , True),\n",
    "    'norm' : (nn.BatchNorm2d , nn.BatchNorm2d , nn.BatchNorm2d),\n",
    "    'activation' : (nn.ReLU , nn.ReLU , nn.ReLU)\n",
    "}\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self , \n",
    "    in_channels : int,\n",
    "    out_channels : int,\n",
    "    stride = 1,\n",
    "    kernel_size = 3,\n",
    "    groups = 1,\n",
    "    expand_ratio = 1,\n",
    "    use_bias  = parameters_dict_ResB['bias'],\n",
    "    norm  = parameters_dict_ResB['norm'],\n",
    "    activation  = parameters_dict_ResB['activation']):\n",
    "        \n",
    "        super(ResBlock, self).__init__()\n",
    "        # print(use_bias)\n",
    "\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "        norm = val2tuple(norm, 2)\n",
    "        activation = val2tuple(activation, 2)\n",
    "        \n",
    "        mid_channel = round(in_channels*expand_ratio)\n",
    "        self.Conv1 = ConvLayer(\n",
    "                in_channels,\n",
    "                mid_channel,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "            \n",
    "                groups = groups,\n",
    "                norm = norm[0],\n",
    "                activation = activation[0],\n",
    "                use_bias=use_bias[0],\n",
    "            )\n",
    "        self.Conv2 = ConvLayer(\n",
    "                mid_channel,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "             \n",
    "                groups = groups,\n",
    "                norm = norm[1],\n",
    "                activation = activation[1],\n",
    "                use_bias=use_bias[1]\n",
    "            )\n",
    "    def forward(self , x : torch.Tensor):\n",
    "        first = self.Conv1(x)\n",
    "        second =self.Conv2(first)\n",
    "    \n",
    "        return second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "434ba8d3-63a0-477e-ba4c-c20b667e1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "    input_shape : int,\n",
    "    output_shape : int,\n",
    "    dropout = 0.3,\n",
    "    use_bias = True,\n",
    "    norm = nn.LayerNorm,\n",
    "    activation = GELUTanh):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
    "        self.linear = nn.Linear(input_shape ,output_shape , bias = use_bias)\n",
    "        \n",
    "        #norm and #activation done later \n",
    "        self.norm = norm(output_shape)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def squeeze(self , x : torch.tensor):\n",
    "        if x.item().dim() > 2 :\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "        return x\n",
    "\n",
    "    def forward(self , x : torch.tensor):\n",
    "        \n",
    "        x = self.squeeze(x)\n",
    "        if self.dropout is not None :\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7947cb-e54f-4839-a94a-90b69990dfcf",
   "metadata": {},
   "source": [
    "## My mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20fd1801-7329-49d3-9143-9719fa7d9dc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(device)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90fc1216-05d9-48ad-a955-802794d3b7fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mMyModule\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m      7\u001B[0m             in_channels,\n\u001B[1;32m      8\u001B[0m             activation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m             \n\u001B[1;32m     15\u001B[0m             ):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8d266d-2e97-4f17-b9f3-ef0bd1155c23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mIdentityLayer\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m , x : torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class IdentityLayer(nn.Module):\n",
    "    def forward(self , x : torch.Tensor):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d15cbd51-f5ec-4426-8930-9db2df2a98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LiteMLA(nn.Module):\n",
    "#     def __init__(self , \n",
    "#     in_channels,\n",
    "#     out_channels,\n",
    "#     heads = None,\n",
    "#     heads_ratio = 1.0,\n",
    "#     dimention=8,\n",
    "#     use_bias=False,\n",
    "#     norm=(None, nn.BatchNorm2d),\n",
    "#     activation=(None, None),\n",
    "#     kernel_func=nn.ReLU,\n",
    "#     scales: tuple[int, ...] = (5,),\n",
    "#     eps=1.0e-15,\n",
    "#     ):\n",
    "\n",
    "        \n",
    "#         super(LiteMLA, self).__init__()\n",
    "#         heads = heads or (in_channels//(dimention * heads_ratio))\n",
    "#         total_dimentions = int(dimention * heads)\n",
    "#         self.eps = eps\n",
    "\n",
    "#         use_bias = val2tuple(use_bias, 2)\n",
    "#         norm = val2tuple(norm, 2)\n",
    "#         activation = val2tuple(activation, 2)\n",
    "        \n",
    "#         self.dim = dimention\n",
    "#         # print(norm)\n",
    "#         # print(in_channels , int(3*total_dimentions) , norm[0],None,use_bias)\n",
    "#         self.qkv = ConvLayer(\n",
    "#                 in_channels = in_channels,\n",
    "#                 out_channels = 3 * total_dimentions,\n",
    "#                 kernel_size=1,\n",
    "                \n",
    "#                 norm = norm[0],\n",
    "#                 activation = activation[0],\n",
    "#                 use_bias=use_bias[0],\n",
    "#             )\n",
    "\n",
    "#         print(scales)\n",
    "#         self.aggregate = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 ConvLayer(\n",
    "#                 3 * total_dimentions,\n",
    "#                 3 * total_dimentions,\n",
    "#                 kernel_size=scale,\n",
    "#                 groups = 3 * total_dimentions,\n",
    "                \n",
    "#                 use_bias=use_bias[0],\n",
    "#             ),\n",
    "#                 ConvLayer(\n",
    "#                 3 * total_dimentions,\n",
    "#                 3 * total_dimentions,\n",
    "#                 kernel_size=1,\n",
    "#                 groups = int(3 * heads),\n",
    "                \n",
    "#                 use_bias=use_bias[0],\n",
    "#             )  \n",
    "                   \n",
    "#             )\n",
    "#             for scale in scales\n",
    "#         ])\n",
    "\n",
    "#         self.activation = kernel_func(inplace=False)\n",
    "#         self.projection = ConvLayer(\n",
    "#                 total_dimentions * (1 + len(scales)),\n",
    "#                 out_channels,\n",
    "#                 kernel_size=1,\n",
    "                \n",
    "#                 norm = norm[1],\n",
    "#                 activation = activation[1],\n",
    "#                 use_bias=use_bias[1],\n",
    "#             )  \n",
    "#     @autocast(enabled=False)\n",
    "#     def ReLU_linear_activation(self,\n",
    "#                               qkv : torch.Tensor):\n",
    "#         B, _, H, W = list(qkv.size())\n",
    "\n",
    "#         if qkv.dtype == torch.float16:\n",
    "#             qkv = qkv.float()\n",
    "\n",
    "#         qkv_reshaped = torch.reshape(qkv , (B,-1,3*self.dim,H*W))\n",
    "#         q, k, v = (\n",
    "#             qkv[:, :, 0 : self.dim],\n",
    "#             qkv[:, :, self.dim : 2 * self.dim],\n",
    "#             qkv[:, :, 2 * self.dim :],\n",
    "#         )\n",
    "#         q = self.activation(q)\n",
    "#         k = self.activation(k)\n",
    "        \n",
    "#         trans_k = k.transpose(-1, -2)\n",
    "\n",
    "#         v = F.pad(v, (0, 0, 0, 1), mode=\"constant\", value=1)\n",
    "#         vk = torch.matmul(v, trans_k)\n",
    "#         out = torch.matmul(vk, q)\n",
    "#         if out.dtype == torch.bfloat16:\n",
    "#             out = out.float()\n",
    "#         out = out[:, :, :-1] / (out[:, :, -1:] + self.eps)\n",
    "\n",
    "#         out = torch.reshape(out, (B, -1, H, W))\n",
    "#         return out\n",
    "\n",
    "#     def forward(self , x : torch.Tensor):\n",
    "#         qkv = self.qkv(x)\n",
    "#         multi_scale_qkv = [qkv]\n",
    "#         for op in self.aggregate:\n",
    "#             multi_scale_qkv.append(op(qkv))\n",
    "#         qkv = torch.cat(multi_scale_qkv, dim=1)\n",
    "#         out = self.ReLU_linear_activation(qkv)\n",
    "#         out = self.projection(out)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee654670-c5be-4b3e-9dd5-0daedcb723a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientViTModule(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels,\n",
    "                dimention=32,\n",
    "                heads_ratio= 1.0,\n",
    "                expand_ratio: float = 4,\n",
    "                scales=(5,),\n",
    "                norm=nn.BatchNorm2d,\n",
    "                activation=\"hswish\",\n",
    "            ):\n",
    "        super(EfficientViTModule, self).__init__()\n",
    "        self.context_module = ResidualBlock(\n",
    "            LiteMLA(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = in_channels,\n",
    "            dimention = dimention,\n",
    "            heads_ratio = heads_ratio,\n",
    "            scales = scales,\n",
    "            norm=(None , norm)), IdentityLayer())\n",
    "        \n",
    "        local_mod = MobileBottleneckConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            expand_ratio=expand_ratio,\n",
    "            use_bias=(True, True, False),\n",
    "            norm=(None, None, norm),\n",
    "            activation=(activation, activation, None),\n",
    "        )\n",
    "        self.local_module = ResidualBlock(local_mod, IdentityLayer())\n",
    "\n",
    "    def forward(self , x:torch.Tensor):\n",
    "        x = self.context_module(x)\n",
    "        x = self.local_module(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941ea156-741a-4b01-b828-5c0265b2b0d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mResidualBlock\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m      4\u001B[0m         main: Optional[nn\u001B[38;5;241m.\u001B[39mModule],\n\u001B[1;32m      5\u001B[0m         shortcut: Optional[nn\u001B[38;5;241m.\u001B[39mModule] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m      6\u001B[0m         pre_norm: Optional[nn\u001B[38;5;241m.\u001B[39mModule] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m      7\u001B[0m     ):\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;28msuper\u001B[39m(ResidualBlock, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        main: Optional[nn.Module],\n",
    "        shortcut: Optional[nn.Module] = None,\n",
    "        pre_norm: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.pre_norm = pre_norm if pre_norm is not None else nn.Identity()\n",
    "        self.main = main\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.main(self.pre_norm(x))\n",
    "        if self.shortcut is not None:\n",
    "            # # print(x.shape)\n",
    "            # print(res.shape)\n",
    "            short = self.shortcut(x)\n",
    "            # print(\"short\" , short.shape)\n",
    "            res = res + short\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a6513a-11e3-4f37-9664-456ec1cc055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class OpSequential(nn.Module):\n",
    "#     def __init__(self, op_list: list[nn.Module or None]):\n",
    "#         super(OpSequential, self).__init__()\n",
    "#         valid_op_list = []\n",
    "#         for op in op_list:\n",
    "#             if op is not None:\n",
    "#                 valid_op_list.append(op)\n",
    "#         self.op_list = nn.ModuleList(valid_op_list)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         for op in self.op_list:\n",
    "#             x = op(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "91aa8294-5fc9-4cd6-a1d7-fffa64df166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation funciton \n",
    "dict = {\n",
    "    'LeakyReLU':nn.LeakyReLU,\n",
    "    'Hardswish':nn.Hardswish,\n",
    "    nn.ReLU:nn.ReLU,\n",
    "    'ReLU6':nn.ReLU6,\n",
    "    GELUTanh:GELUTanh,\n",
    "    'Sigmoid':nn.Sigmoid,\n",
    "    'SiLU':nn.SiLU,\n",
    "}\n",
    "\n",
    "def act_fun( x : str) -> nn.Module:\n",
    "    if x in dict:\n",
    "        print('ehy' , x)\n",
    "        return dict[x]\n",
    "    else:\n",
    "        return nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "524bc1d6-04c8-4817-acc1-f0fea779b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization \n",
    "\n",
    "dict_norm = {\n",
    "    nn.BatchNorm2d : nn.BatchNorm2d,\n",
    "    nn.LayerNorm : nn.LayerNorm\n",
    "}\n",
    "\n",
    "def norm_fun( x : str):\n",
    "    if x in dict_norm:\n",
    "        \n",
    "        return dict_norm[x]\n",
    "    else:\n",
    "        return nn.Identity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c47c2a-9293-422d-b69d-03616d10aaf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73001b1-9bc8-4147-a315-e2640ffe54da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# ## Backbone \u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mMyModule\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      4\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m      9\u001B[0m             in_channels,\n\u001B[1;32m     10\u001B[0m             out_channels,\n\u001B[1;32m     11\u001B[0m             heads \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m     12\u001B[0m             dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32\u001B[39m,\n\u001B[1;32m     13\u001B[0m             ):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# # ## Backbone \n",
    "\n",
    "# class MyModule(nn.Module):\n",
    "#     '''\n",
    "    \n",
    "#     '''\n",
    "    \n",
    "#     def __init__(self,\n",
    "#             in_channels,\n",
    "#             out_channels,\n",
    "#             heads = 10,\n",
    "#             dim = 32,\n",
    "#             ):\n",
    "        \n",
    "#         super(MyModule, self).__init__()\n",
    "#         self.heads = heads\n",
    "#         self.dim = dim\n",
    "#         scale = dim\n",
    "#         self.scale = dim ** -0.5\n",
    "\n",
    "#         # all the 3 conv\n",
    "#         self.total_layer = 4\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "#         self.get_qkv = ConvLayer(in_channels , self.dim * 3  , 3 , norm = nn.BatchNorm2d)\n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "#         self.local_mod = MobileBottleneckConvLayer(out_channels , out_channels , 3 , norm = nn.BatchNorm2d)\n",
    "       \n",
    "\n",
    "        \n",
    "#         self.proj =  ConvLayer(self.dim  , out_channels , 3 , norm = nn.BatchNorm2d)\n",
    "        \n",
    "#         # self.multilpier = ConvLayer(self.dim  , out_channels , 3 , norm = nn.BatchNorm2d)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "#     def attention(self,q,k,v):\n",
    "\n",
    "        \n",
    "        \n",
    "#         q, k, v = q.float(), k.float(), v.float()\n",
    "\n",
    "#         print(\"1\" , q.shape , k.shape , v.shape)\n",
    "\n",
    "#         q = q * self.scale\n",
    "#         att_map = q.transpose(-2, -1) @ k\n",
    "\n",
    "       \n",
    "#         att_map = att_map.softmax(dim=-1)\n",
    "\n",
    "       \n",
    "#         out = v @ att_map.transpose(-2, -1)\n",
    "\n",
    "    \n",
    "        \n",
    "#         return out\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         B, C, H, W = x.shape\n",
    "\n",
    "\n",
    "        \n",
    "#             # print(op)\n",
    "\n",
    "            \n",
    "\n",
    "#         qkv  = self.get_qkv(x)\n",
    "    \n",
    "#         qkv = qkv.reshape(B, -1, 3 * self.dim, H * W).transpose(-1, -2)\n",
    "#         print(qkv.shape)\n",
    "        \n",
    "#         q, k, v = qkv.chunk(3, dim=-1)\n",
    "#         out = self.attention(q , k , v)\n",
    "#         out = out.view(B, self.dim, H, W)\n",
    "      \n",
    "#         # out = F.interpolate(x, size=( H , W ), mode='bilinear')\n",
    "\n",
    "#         # print(\"out1\" , out.shape)\n",
    "        \n",
    "#       # we need to billinear intterpolate before append\n",
    " \n",
    "#         y = self.proj(out)\n",
    "\n",
    "#         y = self.local_mod(y)\n",
    "        \n",
    "#         return y\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # class EfficientViTBackboneLarge(nn.Module):\n",
    "# #     def __init__(self,\n",
    "# #                 width : list[int] ,\n",
    "# #                 depth : list[int] ,\n",
    "# #                 block_list : list[str] or None = None,\n",
    "# #                 expand_list: list[float] = [1, 4, 4, 4, 6],\n",
    "# #                 in_channels=3,\n",
    "# #                 fewer_norm_list: list[bool] or None = [False, False, False, True, True],\n",
    "# #                 qkv_dim=32,\n",
    "# #                 norm=nn.BatchNorm2d,\n",
    "# #                 activation=\"gelu\",\n",
    "            \n",
    "# #                 ):\n",
    "# #         super().__init__()\n",
    "\n",
    "# #         block_list = block_list or [\"res\", \"fmb\", \"fmb\", \"mb\", \"att\"]\n",
    "# #         self.width_list = []\n",
    "# #         self.stages = [] #this is the main model lineup\n",
    "\n",
    "# #         stage0 = [\n",
    "# #             ConvLayer(\n",
    "# #                 in_channels=3,\n",
    "# #                 out_channels=width[0],\n",
    "# #                 stride=2,\n",
    "# #                 norm=norm,\n",
    "# #                 activation=activation,\n",
    "# #             )\n",
    "# #         ]\n",
    "\n",
    "# #         #here more blocks are added to the stage0\n",
    "\n",
    "# #         for _ in range(depth[0]):\n",
    "# #             include_stage0 = self.make_block(\n",
    "# #                 block= block_list[0],\n",
    "# #                 in_channels=width[0],\n",
    "# #                 out_channels=width[0],\n",
    "# #                 stride=1,\n",
    "# #                 expand_ratio=expand_list[0],\n",
    "# #                 norm=norm,\n",
    "# #                 activation=activation,\n",
    "# #                 fewer_norm=fewer_norm_list[0],\n",
    "# #             )\n",
    "# #             stage0.append(ResidualBlock(include_stage0, IdentityLayer()))\n",
    "# #         self.stages.append(OpSequential(stage0))\n",
    "            \n",
    "# #             #now we make multiple more stages\n",
    "# #         in_channels = width[0]\n",
    "       \n",
    "# #         for stage_id, (w,d) in enumerate(zip(width[1:] ,depth[1:] ), start = 1):\n",
    "# #             #make a block\n",
    "           \n",
    "# #             stage = []\n",
    "# #             # print(block_list[stage_id])\n",
    "# #             block = self.make_block(\n",
    "# #                 block=\"mb\" if block_list[stage_id] not in [\"mb\", \"fmb\"] else block_list[stage_id],\n",
    "# #                 in_channels=in_channels,\n",
    "# #                 out_channels=w,\n",
    "# #                 stride=2,\n",
    "# #                 expand_ratio=expand_list[stage_id] * 4,\n",
    "# #                 norm=norm,\n",
    "# #                 activation=activation,\n",
    "# #                 fewer_norm=fewer_norm_list[stage_id],\n",
    "# #             )\n",
    "# #             stage.append(ResidualBlock(block, None))\n",
    "# #             # print(w)\n",
    "# #             in_channels = w\n",
    "# #             for _ in range(d):\n",
    "# #                 #if att make a EffViTblock else MBConv\n",
    "# #                 if block_list[stage_id].startswith(\"att\"):\n",
    "# #                     stage.append(\n",
    "# #                         EfficientViTModule(\n",
    "# #                             in_channels=in_channels,\n",
    "# #                             dimention=qkv_dim,\n",
    "# #                             expand_ratio=expand_list[stage_id],\n",
    "# #                             scales=(3) if block_list[stage_id] == \"att@3\" else (5,),\n",
    "# #                             norm=(norm,norm),\n",
    "# #                             activation=activation,\n",
    "# #                         )\n",
    "# #                     )\n",
    "# #                 else:\n",
    "# #                     block = self.make_block(\n",
    "# #                         block=block_list[stage_id],\n",
    "# #                         in_channels=in_channels,\n",
    "# #                         out_channels=in_channels,\n",
    "# #                         stride=1,\n",
    "# #                         expand_ratio=expand_list[stage_id],\n",
    "# #                         norm=norm,\n",
    "# #                         activation=activation,\n",
    "# #                         fewer_norm=fewer_norm_list[stage_id],\n",
    "# #                     )\n",
    "# #                     block = ResidualBlock(block, IdentityLayer())\n",
    "# #                     stage.append(block)\n",
    "# #                     # print(\"stage\" , stage)\n",
    "# #             self.stages.append(OpSequential(stage))\n",
    "# #         self.stages = nn.ModuleList(self.stages)\n",
    "# #     @staticmethod\n",
    "# #     def make_block(\n",
    "# #         block: str,\n",
    "# #         in_channels: int,\n",
    "# #         out_channels: int,\n",
    "# #         stride: int,\n",
    "# #         expand_ratio: float,\n",
    "# #         norm: str,\n",
    "# #         activation: str,\n",
    "# #         fewer_norm: bool = False,\n",
    "# #         ) -> nn.Module:\n",
    "# #         if block == \"res\":\n",
    "# #             block = ResBlock(\n",
    "# #                 in_channels=in_channels,\n",
    "# #                 out_channels=out_channels,\n",
    "# #                 stride=stride,\n",
    "# #                 use_bias=(True, False) if fewer_norm else False,\n",
    "# #                 norm=(None, norm) if fewer_norm else norm,\n",
    "# #                 activation=(activation, None),\n",
    "# #             )\n",
    "# #         elif block == \"fmb\":\n",
    "# #             print(\"its FNB\")\n",
    "# #             block = FusedMobileBottleneckConvLayer(\n",
    "# #                 in_channels=in_channels,\n",
    "# #                 out_channels=out_channels,\n",
    "# #                 stride=stride,\n",
    "# #                 expand_ratio=expand_ratio,\n",
    "# #                 use_bias=(True, False) if fewer_norm else False,\n",
    "# #                 norm=(None, norm) if fewer_norm else norm,\n",
    "# #                 activation=(activation, None),\n",
    "# #             )\n",
    "# #         elif block == \"mb\":\n",
    "# #             block = MobileBottleneckConvLayer(\n",
    "# #                 in_channels=in_channels,\n",
    "# #                 out_channels=out_channels,\n",
    "# #                 stride=stride,\n",
    "# #                 expand_ratio=expand_ratio,\n",
    "# #                 use_bias=(True, True, False) if fewer_norm else False,\n",
    "# #                 norm=(None, None, norm) if fewer_norm else norm,\n",
    "# #                 activation=(activation, activation, None),\n",
    "# #             )\n",
    "# #         else:\n",
    "# #             raise ValueError(block)\n",
    "# #         return block\n",
    "\n",
    "# #     def forward(self, x: torch.Tensor) :\n",
    "# #         output_dict = {\"input\": x}\n",
    "# #         for stage_id, stage in enumerate(self.stages):\n",
    "# #             output_dict[\"stage%d\" % stage_id] = x = stage(x)\n",
    "# #         output_dict[\"stage_final\"] = x\n",
    "# #         return output_dict\n",
    "\n",
    "\n",
    "# # def efficientvit_backbone_l1(**kwargs) -> EfficientViTBackboneLarge:\n",
    "# #     backbone = EfficientViTBackboneLarge(\n",
    "# #         width=[32, 64, 128, 256, 512],\n",
    "# #         depth=[1, 1, 1, 6, 6],\n",
    "# #     )\n",
    "# #     return backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "248182f7-fdf0-4409-aaad-fa7fa96b7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mkaing the complete model from scratch efficientVIT.l1\n",
    "\n",
    "# def make_efficientViTl1():\n",
    "\n",
    "#     stage = []\n",
    "\n",
    "#     conv0 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da07e6de-15af-44d9-b8f3-501cc17d554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientViTBackboneLarge(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                 width=[32, 64, 128, 256, 512],\n",
    "#                 depth :  depth=[1, 1, 1, 6, 6] ,\n",
    "                \n",
    "#                 expand_list: list[float] = [1, 4, 4, 4, 6],\n",
    "#                 in_channels=3,\n",
    "#                 fewer_norm_list: list[bool] or None = [False, False, False, True, True],\n",
    "#                 qkv_dim=32,\n",
    "#                 norm=nn.BatchNorm2d,\n",
    "#                 activation=\"gelu\",\n",
    "            \n",
    "#                 ):\n",
    "#         self.stage = []\n",
    "\n",
    "#         self.stem = [\n",
    "            \n",
    "#         ]\n",
    "\n",
    "#         conv0 = self.make_block(\n",
    "#                 block = 'conv',\n",
    "#                 in_channels=3,\n",
    "#                 out_channels=width[0],\n",
    "#                 stride=2,\n",
    "#                 norm=norm,\n",
    "#                 activation=activation,\n",
    "#             )\n",
    "\n",
    "#         res0 = stage.append(ResidualBlock(self.make_block('res',in_channels =width[0], \n",
    "#                                                           out_channels=width[0] , \n",
    "#                                                           stride=1,\n",
    "#                                                           expand_ratio=expand_list[0],\n",
    "#                                                           norm=norm,\n",
    "#                                                           activation=activation,\n",
    "#                                                           fewer_norm=fewer_norm_list[0],), IdentityLayer()))\n",
    "\n",
    "#         self.stem.append(conv0)\n",
    "#         self.stem.append(res0)\n",
    "        \n",
    "#         #STEM DONE\n",
    "\n",
    "#         #woking on VITblocks\n",
    "        \n",
    "        \n",
    "\n",
    "#         self.EfficintViTModules = []\n",
    "\n",
    "        \n",
    "\n",
    "#         #efficentvitmodule1\n",
    "\n",
    "#         EfficintViTModule0 = self.make_res_blocks( [[block = 'fmbconv' , in_channels=width[0] ,out_channels= width[1], stride = 2 ,expand_ratio=expand_list[1]*4,scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ],\n",
    "#                                                     [block = 'fmbconv' ,in_channels=width[1] ,out_channels= width[1],stride = 1,expand_ratio=expand_list[1],scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ]])\n",
    "\n",
    "#         EfficintViTModule1 = self.make_res_blocks([[block = 'fmbconv' , in_channels=width[1] ,out_channels= width[2], stride = 2 ,expand_ratio=expand_list[1]*4,scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ],\n",
    "#                                                     [block = 'fmbconv' ,in_channels=width[2] ,out_channels= width[2],stride = 1,expand_ratio=expand_list[1],scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ]])\n",
    "\n",
    "#         EfficintViTModule2 = self.make_res_blocks([[block = 'mbconv' , in_channels=width[2] ,out_channels= width[3], stride = 2 ,expand_ratio=expand_list[2],scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ],\n",
    "#                                                     [block = 'mbconv' ,in_channels=width[3] ,out_channels= width[3],stride = 1,expand_ratio=expand_list[2],scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ],\n",
    "#                                                     [block = 'mbconv' ,in_channels=width[3] ,out_channels= width[3],stride = 1,expand_ratio=expand_list[2],scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ],\n",
    "#                                                     [block = 'mbconv' ,in_channels=width[3] ,out_channels= width[3],stride = 1,expand_ratio=expand_list[2],scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ],\n",
    "#                                                     [block = 'mbconv' , in_channels=width[2] ,out_channels= width[3], stride = 1 ,expand_ratio=expand_list[2],scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ],\n",
    "#                                                     [block = 'mbconv' ,in_channels=width[3] ,out_channels= width[3],stride = 1,expand_ratio=expand_list[2],scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ],\n",
    "#                                                     [block = 'mbconv' ,in_channels=width[3] ,out_channels= width[3],stride = 1,expand_ratio=expand_list[2],scales=(3),norm=(norm,norm),\n",
    "#                             activation=activation ],])\n",
    "\n",
    "#         EfficintViTModule3 = self.make_res_blocks([block = 'mbconv' , in_channels=width[3] ,out_channels= width[4], stride = 2 ,expand_ratio=expand_list[4]*4,scales=(3),norm=(norm,norm),\n",
    "#                                 activation=activation ])\n",
    "\n",
    "        \n",
    "        \n",
    "#         att1 = self.make_block('eff' ,in_channels =width[4], dimention=qkv_dim, expand_ratio = expand_list[4] , scales=(3),\n",
    "#                                 norm=(norm,norm),\n",
    "#                                 activation=activation, )\n",
    "#         att2 = self.make_block('eff' ,in_channels =width[4], dimention=qkv_dim, expand_ratio = expand_list[4] , scales=(3),\n",
    "#                                 norm=(norm,norm),\n",
    "#                                 activation=activation, )\n",
    "        \n",
    "#         EfficintViTModule3.extend([att1 , att1 ,att1 ,att1 ,att1 ,att1 ])\n",
    "#         self.EfficintViTModules.extend([EfficintViTModule0,EfficintViTModule1,EfficintViTModule2,EfficintViTModule3 ])\n",
    "\n",
    "#         #classifier head \n",
    "#         classifier = self.make_block(\n",
    "#                 block = 'conv'\n",
    "#                 in_channels=width[4],\n",
    "#                 out_channels=width[4]*6,\n",
    "#                 stride=1,\n",
    "#                 norm=norm,\n",
    "#                 activation=activation,\n",
    "#             )\n",
    "        \n",
    "#         pool = torch.nn.AdaptiveAvgPool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1)\n",
    "#         classifier.append(pool)\n",
    "\n",
    "#         seq = torch.nn.Sequential(\n",
    "#             LinearLayer(width[4]*6 , 3200 , 0 , use_bias = False),\n",
    "#             LinearLayer(3200 , 1000 , 0 , use_bias = False , None , None)\n",
    "#         )\n",
    "#         classifier.append(seq)\n",
    "\n",
    "#         #make model\n",
    "\n",
    "#         self.stage.append(self.stem)\n",
    "#         self.stage.append(self.EfficintViTModules)\n",
    "#         self.stage.append(self.classifier)\n",
    "#         return self.stage\n",
    "\n",
    "#     def make_block(self,**kwargs):\n",
    "#         if block == \"conv\":\n",
    "#             blc = ConvLayer(\n",
    "#                     in_channels=in_channels,\n",
    "#                     out_channels=out_channels,\n",
    "#                     stride=stride,\n",
    "#                     norm= None,\n",
    "#                     use_bias=False\n",
    "#                     activation=None,\n",
    "#                 )\n",
    "#         elif block == \"res\":\n",
    "#             blc = ResBlock(\n",
    "#                     in_channels=in_channels,\n",
    "#                     out_channels=out_channels,\n",
    "#                     stride=stride,\n",
    "#                     use_bias=(True, False) if fewer_norm else False,\n",
    "#                     norm=(None, norm) if fewer_norm else norm,\n",
    "#                     activation=(activation, None),\n",
    "#                 )\n",
    "        \n",
    "#         elif block == \"fmb\":\n",
    "           \n",
    "#             blc = FusedMobileBottleneckConvLayer(\n",
    "#                 in_channels=in_channels,\n",
    "#                 out_channels=out_channels,\n",
    "#                 stride=stride,\n",
    "#                 expand_ratio=expand_ratio,\n",
    "#                 use_bias=(True, False) if fewer_norm else False,\n",
    "#                 norm=(None, norm) if fewer_norm else norm,\n",
    "#                 activation=(activation, None),\n",
    "#             )\n",
    "#         elif block == \"mb\":\n",
    "#             blc = MobileBottleneckConvLayer(\n",
    "#                 in_channels=in_channels,\n",
    "#                 out_channels=out_channels,\n",
    "#                 stride=stride,\n",
    "#                 expand_ratio=expand_ratio,\n",
    "#                 use_bias=(True, True, False) if fewer_norm else False,\n",
    "#                 norm=(None, None, norm) if fewer_norm else norm,\n",
    "#                 activation=(activation, activation, None),\n",
    "#             )\n",
    "#         elif block == 'eff':\n",
    "#             blc = EfficientViTModule(\n",
    "#                     in_channels=in_channels,\n",
    "#                     dimention=qkv_dim,\n",
    "#                     expand_ratio=expand_ratio,\n",
    "#                     scales=(3),\n",
    "#                     norm=(norm,norm),\n",
    "#                     activation=activation,\n",
    "#                             )\n",
    "#         return blc\n",
    "\n",
    "#     def make_modules(blocks , residual = True):\n",
    "#         all = []\n",
    "#         for i in range(len(block)):\n",
    "#             all.append(ResidualBlock(self.make_block(blocks[i]), IdentityLayer())))\n",
    "#         return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c39e00-5a03-4e90-8b2f-81e723c7b9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
