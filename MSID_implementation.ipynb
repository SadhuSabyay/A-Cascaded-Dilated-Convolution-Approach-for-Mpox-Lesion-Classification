{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ecd3f3-eed2-4a9a-9b07-2dd4c5534e85",
   "metadata": {
    "id": "24ecd3f3-eed2-4a9a-9b07-2dd4c5534e85"
   },
   "source": [
    "# Mpox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc3ad59-fdc9-4094-9083-f8ff5539cbec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfc3ad59-fdc9-4094-9083-f8ff5539cbec",
    "outputId": "c6ed3171-5b88-466c-900f-d4a824019593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision.models as tvm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "import torch.optim as to\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "\n",
    "%run model.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OoaL604Rk4Ns",
   "metadata": {
    "id": "OoaL604Rk4Ns"
   },
   "outputs": [],
   "source": [
    "!unzip '/Monkeypox Skin Image Dataset-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QqTT4PAiHadt",
   "metadata": {
    "id": "QqTT4PAiHadt"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db07df5-0b5d-450b-b882-19787ec54760",
   "metadata": {
    "id": "0db07df5-0b5d-450b-b882-19787ec54760"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6bdf19-78f5-4465-b546-c6314ccb3549",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce6bdf19-78f5-4465-b546-c6314ccb3549",
    "outputId": "15a82116-8674-40fa-a557-36a5f11373f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chickenpox': 0, 'Measles': 1, 'Monkeypox': 2, 'Normal': 3}\n",
      "\n",
      "Dataset Statistics:\n",
      "X_train shape: torch.Size([616, 3, 224, 224])\n",
      "X_test shape: torch.Size([154, 3, 224, 224])\n",
      "y_train shape: torch.Size([616, 4])\n",
      "y_test shape: torch.Size([154, 4])\n",
      "\n",
      "Class distribution:\n",
      "Chickenpox: 86.0 train, 21.0 test\n",
      "Measles: 73.0 train, 18.0 test\n",
      "Monkeypox: 223.0 train, 56.0 test\n",
      "Normal: 234.0 train, 59.0 test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # For GPU reproducibility\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "def load_msid_data(data_dir, img_size=224):\n",
    "    # Define the classes\n",
    "    classes = ['Chickenpox', 'Measles', 'Monkeypox', 'Normal']\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "    print(class_to_idx)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # Create lists to store paths and labels\n",
    "    all_paths = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Collect all image paths and labels\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        class_idx = class_to_idx[class_name]\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                all_paths.append(img_path)\n",
    "                all_labels.append(class_idx)\n",
    "\n",
    "    # Split the data while maintaining class proportions\n",
    "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "        all_paths, all_labels,\n",
    "        test_size=0.2,\n",
    "        stratify=all_labels,\n",
    "        random_state=SEED  # Seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Define transform - only resize and convert to tensor, then divide by 255\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),  # ToTensor automatically divides by 255\n",
    "    ])\n",
    "\n",
    "    # Convert images to tensors\n",
    "    def paths_to_tensor(image_paths):\n",
    "        tensors = []\n",
    "        for img_path in image_paths:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            tensor = transform(image)\n",
    "            tensors.append(tensor)\n",
    "        return torch.stack(tensors)\n",
    "\n",
    "    # Create one-hot encoded labels\n",
    "    def to_one_hot(labels, num_classes):\n",
    "        labels_tensor = torch.tensor(labels)\n",
    "        one_hot = torch.zeros(len(labels), num_classes)\n",
    "        one_hot.scatter_(1, labels_tensor.unsqueeze(1), 1)\n",
    "        return one_hot\n",
    "\n",
    "    # Create tensors\n",
    "    X_train = paths_to_tensor(train_paths)\n",
    "    X_test = paths_to_tensor(test_paths)\n",
    "    y_train = to_one_hot(train_labels, num_classes)\n",
    "    y_test = to_one_hot(test_labels, num_classes)\n",
    "\n",
    "    # Print dataset statistics\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "    print(\"\\nClass distribution:\")\n",
    "    for cls in classes:\n",
    "        train_count = y_train[:, class_to_idx[cls]].sum().item()\n",
    "        test_count = y_test[:, class_to_idx[cls]].sum().item()\n",
    "        print(f\"{cls}: {train_count} train, {test_count} test\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, classes\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"Monkeypox Skin Image Dataset-2\"\n",
    "    X_train, y_train, X_test, y_test, classes = load_msid_data(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9e9770-bec6-432e-aaf5-651e7674357c",
   "metadata": {
    "id": "6e9e9770-bec6-432e-aaf5-651e7674357c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "169f9c9f-df3d-4853-a46d-7a8f717d1b25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "169f9c9f-df3d-4853-a46d-7a8f717d1b25",
    "outputId": "06d56edf-81aa-46a0-9819-70520712e860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.12\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "print(timm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e34ba-11f7-4dc4-89f3-60e1cde88227",
   "metadata": {
    "id": "4b6e34ba-11f7-4dc4-89f3-60e1cde88227",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.save('X_train.npy', X_train)\n",
    "# np.save('y_train.npy', y_train)\n",
    "# np.save('X_test.npy', X_test)\n",
    "# np.save('y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54bd7adf-ca24-465f-8509-95f51cb985a3",
   "metadata": {
    "id": "54bd7adf-ca24-465f-8509-95f51cb985a3"
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a classification model using precision, recall, and F1-score.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (list or array-like): True labels.\n",
    "    y_pred (list or array-like): Predicted labels by the model.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Precision, Recall, and F1-Score, calculated using macro averaging.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate precision score using macro averaging (treats all classes equally)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Calculate recall score using macro averaging\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Calculate F1-score using macro averaging\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Return the calculated metrics as a tuple\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc926ec-69bc-4c16-bf11-dace61e6cb2a",
   "metadata": {
    "id": "0fc926ec-69bc-4c16-bf11-dace61e6cb2a",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be10e4-af33-410a-b1fe-c63547799b89",
   "metadata": {
    "id": "52be10e4-af33-410a-b1fe-c63547799b89",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "267f1e3c-672e-4832-9bc0-ec2fe0bff61e",
   "metadata": {
    "id": "267f1e3c-672e-4832-9bc0-ec2fe0bff61e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def replace_context_modules(model, Module, dilation):\n",
    "    \"\"\"\n",
    "    Replaces the context_module in specific blocks of a model's fourth stage with a custom module.\n",
    "\n",
    "    Parameters:\n",
    "    model: The model containing stages and blocks where replacements are to be made.\n",
    "    Module: Custom module to replace the existing context_module.\n",
    "    seed: Random seed for reproducibility in the custom module.\n",
    "    \"\"\"\n",
    "\n",
    "    # Access the fourth stage (index 3) of the model\n",
    "    stage = model.stages[3]\n",
    "\n",
    "    # Loop through blocks 1 to 6 (indices 1 to 6) in the fourth stage\n",
    "    for i in range(1, 7):\n",
    "        block = stage.blocks[i]  # Access the current block\n",
    "\n",
    "        # Extract the number of input channels from the original context_module\n",
    "        in_channels = block.context_module.main.qkv.conv.in_channels\n",
    "\n",
    "        # Extract the number of output channels from the original context_module\n",
    "        out_channels = block.context_module.main.proj.conv.out_channels\n",
    "\n",
    "        # Replace the existing context_module with a new custom module\n",
    "        block.context_module = nn.Sequential(\n",
    "            Module(in_channels, nn.ReLU, dilation=dilation)  # Initialize custom module with required parameters\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "464db3fe-8de7-423f-bbfd-41e7fe6c5965",
   "metadata": {
    "id": "464db3fe-8de7-423f-bbfd-41e7fe6c5965"
   },
   "outputs": [],
   "source": [
    "\n",
    "def change_classifier(model, model_name, num_classes=4, dropout=0.5,\n",
    "                     neurons1=4096, neurons2=1024, neurons3=256, neurons4=512, n_layers=2):\n",
    "    \"\"\"\n",
    "    Change the classifier head of various vision models\n",
    "\n",
    "    Args:\n",
    "        model: The base model to modify\n",
    "        model_name: Name/type of the model to determine input features\n",
    "        num_classes: Number of output classes\n",
    "        dropout: Dropout rate\n",
    "        neurons1-4: Number of neurons in each layer\n",
    "        n_layers: Number of layers in classifier (1-4)\n",
    "    \"\"\"\n",
    "    # Define input features based on model architecture\n",
    "    input_features = {\n",
    "        'resnet101.a1_in1k': 2048,\n",
    "        'deit3_medium_patch16_224': 512,\n",
    "        'coatnet_1_rw_224.sw_in1k': 768,\n",
    "        'mobilenetv3_large_100.ra_in1k': 1280,\n",
    "        'vit_base_patch16_224' : 768,\n",
    "        'efficientvit_l1.r224_in1k': 3072\n",
    "    }\n",
    "\n",
    "    in_features = input_features.get(model_name, 3072)  # Default to 3072 if model not found\n",
    "\n",
    "    # Create the classifier based on number of layers\n",
    "    if n_layers == 1:\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, neurons1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons1, num_classes),\n",
    "            nn.Sigmoid() if num_classes == 1 else nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    elif n_layers == 2:\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, neurons1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons1, neurons2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons2, num_classes),\n",
    "            nn.Sigmoid() if num_classes == 1 else nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    elif n_layers == 3:\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, neurons1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons1, neurons2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons2, neurons3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons3, num_classes),\n",
    "            nn.Sigmoid() if num_classes == 1 else nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    else:  # 4 layers\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, neurons1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons1, neurons2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons2, neurons3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons3, neurons4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(neurons4, num_classes),\n",
    "            nn.Sigmoid() if num_classes == 1 else nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    # Determine where to attach the classifier based on model type\n",
    "    if hasattr(model, 'head'):\n",
    "      if hasattr(model.head, 'classifier'):\n",
    "        model.head.classifier = classifier\n",
    "\n",
    "      elif hasattr(model.head, 'fc'):\n",
    "        model.head.fc = classifier\n",
    "      else:\n",
    "         model.head = classifier\n",
    "    elif hasattr(model, 'fc'):\n",
    "      model.fc = classifier\n",
    "    elif hasattr(model, 'classifier'):\n",
    "      model.classifier = classifier\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise AttributeError(\"Model structure not supported. Cannot find classifier or head attribute.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c13559c-2e87-474d-9769-d84da558d30a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c13559c-2e87-474d-9769-d84da558d30a",
    "outputId": "ace93fec-84c5-491d-dcf4-784c2b6e17c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d68765-0e15-4956-b4d6-d96b8da4f510",
   "metadata": {
    "id": "a3d68765-0e15-4956-b4d6-d96b8da4f510"
   },
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    'Chickenpox': [1, 0, 0, 0],\n",
    "    'Measles': [0, 1, 0, 0],\n",
    "    'Monkeypox': [0, 0, 1, 0],\n",
    "    'Normal': [0, 0, 0, 1]\n",
    "}\n",
    "# def acc_eval(outputs, labels, classwise=False):\n",
    "#     predicted = (outputs > 0.5).float()\n",
    "#     correct = (predicted == labels).float()\n",
    "#     right = correct.sum().item()\n",
    "\n",
    "#     if not classwise:\n",
    "#         return right\n",
    "#     else:\n",
    "#         class_rights = {\n",
    "#             'Other': ((1 - labels) * correct).sum().item(),\n",
    "#             'Monkeypox': (labels * correct).sum().item()\n",
    "#         }\n",
    "#         return (right, class_rights['Other'], class_rights['Monkeypox'])\n",
    "\n",
    "def acc_eval(outputs, labels, classwise=False):\n",
    "    \"\"\"\n",
    "    Evaluate accuracy of model predictions.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model predictions (one-hot encoded).\n",
    "        labels (torch.Tensor): Ground truth labels (one-hot encoded).\n",
    "        classwise (bool): If True, calculate and return class-wise accuracy.\n",
    "\n",
    "    Returns:\n",
    "        int: Total correct predictions if classwise=False.\n",
    "        tuple: Total correct predictions and class-wise counts if classwise=True.\n",
    "    \"\"\"\n",
    "    right = 0  # Counter for correct predictions\n",
    "    class_rights = {class_name: 0 for class_name in class_labels.keys()}  # Initialize class-wise correct counters\n",
    "\n",
    "    for j in range(outputs.shape[0]):  # Loop over all predictions in the batch\n",
    "        max_value = torch.max(outputs[j])  # Get the maximum value in the current prediction\n",
    "        outputs[j] = (outputs[j] == max_value).float()  # Convert to one-hot representation by retaining the max value index\n",
    "        if torch.all(outputs[j].eq(labels[j])):  # Check if the prediction matches the ground truth\n",
    "            right += 1  # Increment total correct counter\n",
    "            if classwise:  # If classwise evaluation is required\n",
    "                label_list = labels[j].detach().cpu().tolist()  # Convert label tensor to list for comparison\n",
    "                for class_name, class_label in class_labels.items():  # Loop through each class label\n",
    "                    if label_list == class_label:  # Check if the ground truth matches the current class label\n",
    "                        class_rights[class_name] += 1  # Increment correct counter for the class\n",
    "                        break  # Exit the loop once the class is identified\n",
    "    if not classwise:  # Return total correct predictions if classwise=False\n",
    "        return right\n",
    "    else:  # Return total correct predictions and class-wise correct counts if classwise=True\n",
    "        return (right, *class_rights.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84713b2a-975b-4bbc-b7a8-ce2390295a7a",
   "metadata": {
    "id": "84713b2a-975b-4bbc-b7a8-ce2390295a7a"
   },
   "outputs": [],
   "source": [
    "def cal_total(labels):\n",
    "    \"\"\"\n",
    "    Calculate the total count of labels for each class.\n",
    "\n",
    "    Args:\n",
    "        labels (torch.Tensor): Ground truth labels (one-hot encoded).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Total counts for each class, in the order of class_labels keys.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store the total count for each class\n",
    "    class_totals = {class_name: 0 for class_name in class_labels.keys()}\n",
    "\n",
    "    for j in range(labels.shape[0]):  # Loop over all labels in the batch\n",
    "        label_list = labels[j].detach().cpu().tolist()  # Convert label tensor to a list\n",
    "        for class_name, class_label in class_labels.items():  # Iterate through all class labels\n",
    "            if label_list == class_label:  # Check if the label matches the current class\n",
    "                class_totals[class_name] += 1  # Increment the count for the matched class\n",
    "                break  # Exit the loop once the class is identified\n",
    "\n",
    "    # Return the total counts for each class as a tuple\n",
    "    return tuple(class_totals.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f6178f-64cd-4f99-bbd3-e90c2303332c",
   "metadata": {
    "id": "51f6178f-64cd-4f99-bbd3-e90c2303332c"
   },
   "outputs": [],
   "source": [
    "def norm(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Normalize training, validation, and test datasets using training set statistics.\n",
    "    \"\"\"\n",
    "    meanx = X_train.mean()  # Calculate training set mean\n",
    "    stdx = X_train.std()    # Calculate training set std\n",
    "\n",
    "    # Normalize datasets using training set mean and std\n",
    "    X_train = (X_train - meanx) / stdx\n",
    "    #X_valid = (X_val - meanx) / stdx\n",
    "    X_test = (X_test - meanx) / stdx\n",
    "\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0542860-41f2-45b3-9c68-425647660d4f",
   "metadata": {
    "id": "e0542860-41f2-45b3-9c68-425647660d4f",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d58417-8fd8-40e4-914b-49ee9f46a3c1",
   "metadata": {
    "id": "b7d58417-8fd8-40e4-914b-49ee9f46a3c1"
   },
   "outputs": [],
   "source": [
    "def test(model, training_loader, model_num):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset and compute accuracy, precision, recall, F1 score,\n",
    "    and per-class accuracies.\n",
    "    \"\"\"\n",
    "    right_total = 0  # Total correct predictions\n",
    "    total = 0  # Total number of samples\n",
    "    out = []  # List to store predicted labels\n",
    "    lab = []  # List to store true labels\n",
    "    class_totals = {class_name: 0 for class_name in class_labels.keys()}  # Per-class sample counts\n",
    "    class_rights = {class_name: 0 for class_name in class_labels.keys()}  # Per-class correct predictions\n",
    "\n",
    "    for i, data in enumerate(tqdm(training_loader)):  # Iterate over the training data\n",
    "        inputs, labels = data  # Get inputs and labels\n",
    "        total += inputs.shape[0]  # Update total samples\n",
    "\n",
    "        outputs = model(inputs)  # Get model predictions\n",
    "\n",
    "        # Get per-class totals and correct predictions\n",
    "        class_totals_batch = cal_total(labels)\n",
    "        right, *class_rights_batch = acc_eval(outputs, labels, classwise=True)\n",
    "\n",
    "        right_total += right  # Update total correct predictions\n",
    "        for i, class_name in enumerate(class_labels.keys()):  # Update per-class totals and rights\n",
    "            class_totals[class_name] += class_totals_batch[i]\n",
    "            class_rights[class_name] += class_rights_batch[i]\n",
    "\n",
    "        # Convert outputs and labels to numpy arrays for evaluation\n",
    "        outputs = np.array(outputs.detach().cpu(), dtype='object')\n",
    "        labels = np.array(labels.detach().cpu(), dtype='object')\n",
    "\n",
    "        out.extend(np.argmax(outputs, axis=1))  # Store predicted labels\n",
    "        lab.extend(np.argmax(labels, axis=1))  # Store true labels\n",
    "\n",
    "    accuracy = right_total / total  # Calculate overall accuracy\n",
    "    class_accuracies = {class_name: class_rights[class_name] / class_totals[class_name]\n",
    "                        if class_totals[class_name] > 0 else 0\n",
    "                        for class_name in class_labels.keys()}  # Calculate per-class accuracies\n",
    "\n",
    "    precision, recall, f1 = evaluate(out, lab)  # Evaluate precision, recall, and F1 score\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)  # Print overall accuracy\n",
    "    print(\"Total Right:\", right_total)  # Print total correct predictions\n",
    "    for class_name, class_accuracy in class_accuracies.items():  # Print per-class accuracy\n",
    "        print(f\"{class_name} Accuracy: {class_accuracy}\")\n",
    "\n",
    "    return (accuracy, precision, recall, f1, *class_accuracies.values())  # Return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edc659e3-125f-4b3f-961c-5175759be21b",
   "metadata": {
    "id": "edc659e3-125f-4b3f-961c-5175759be21b"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, epoch_index, model_num, training_loader, loss_fn, loss_fn1, w, optimizer, loss_dict_train):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch, computing losses and accuracies, and updating model weights.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to train.\n",
    "        epoch_index: The index of the current epoch.\n",
    "        model_num: Identifier for the model (used for loss tracking).\n",
    "        training_loader: DataLoader object for training dataset.\n",
    "        loss_fn: Primary loss function used for training.\n",
    "        loss_fn1: Secondary loss function used for training (combined with loss_fn).\n",
    "        w: Weighting factor for combining loss_fn and loss_fn1.\n",
    "        optimizer: Optimizer to update model parameters.\n",
    "        loss_dict_train: Dictionary to track losses for the training process.\n",
    "\n",
    "    Returns:\n",
    "        last_loss: The average loss for the last batch in the epoch.\n",
    "        overall_accuracy: The overall accuracy of the model on the training dataset.\n",
    "        right_total: Total number of correct predictions in the epoch.\n",
    "        class_accuracies: Per-class accuracy for each class in the dataset.\n",
    "    \"\"\"\n",
    "    running_loss = 0.  # To track cumulative loss for the current epoch\n",
    "    last_loss = 0.  # To store the average loss for the last batch\n",
    "    right_total = 0  # Total correct predictions\n",
    "    total = 0  # Total samples processed\n",
    "\n",
    "    class_totals = {class_name: 0 for class_name in class_labels.keys()}  # Store count of each class in the batch\n",
    "    class_rights = {class_name: 0 for class_name in class_labels.keys()}  # Store correct predictions for each class\n",
    "\n",
    "    # Iterate through batches in the training set\n",
    "    for i, data in enumerate(tqdm(training_loader)):\n",
    "        inputs, labels = data  # Get input images and corresponding labels\n",
    "        optimizer.zero_grad()  # Reset gradients to zero before backpropagation\n",
    "        outputs = model(inputs)  # Get model predictions\n",
    "\n",
    "        total += inputs.shape[0]  # Update the total number of samples processed\n",
    "        # Compute the loss as a weighted combination of the two loss functions\n",
    "        loss = (1-w) * loss_fn(outputs, labels) + w * loss_fn1(outputs, labels)\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update the model weights using the optimizer\n",
    "\n",
    "        # Get per-class totals and correct predictions\n",
    "        class_totals_batch = cal_total(labels)\n",
    "        right, *class_rights_batch = acc_eval(outputs, labels, classwise=True)\n",
    "\n",
    "        right_total += right  # Update the total correct predictions\n",
    "        # Update per-class totals and correct predictions\n",
    "        for j, class_name in enumerate(class_labels.keys()):\n",
    "            class_totals[class_name] += class_totals_batch[j]\n",
    "            class_rights[class_name] += class_rights_batch[j]\n",
    "\n",
    "        running_loss += loss.item()  # Add current batch loss to running total\n",
    "        # Print the average loss for every 10 batches\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10  # Calculate the average loss for the last 10 batches\n",
    "            print(f'  batch {i + 1} loss: {last_loss}')\n",
    "            running_loss = 0.  # Reset running loss for the next set of batches\n",
    "\n",
    "    # Calculate per-class accuracies\n",
    "    print(\"Class totals:\", class_totals)\n",
    "    class_accuracies = {}\n",
    "    for class_name in class_labels.keys():\n",
    "        if class_totals[class_name] == 0:\n",
    "            class_accuracies[class_name] = None  # No data for this class\n",
    "        else:\n",
    "            class_accuracies[class_name] = class_rights[class_name] / class_totals[class_name]  # Calculate class accuracy\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = right_total / total if total > 0 else None\n",
    "\n",
    "    # Return the last loss, overall accuracy, total correct predictions, and class-wise accuracies\n",
    "    return (last_loss, overall_accuracy, right_total,\n",
    "            *[class_accuracies[class_name] for class_name in class_labels.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vKPBSw9V0zcO",
   "metadata": {
    "id": "vKPBSw9V0zcO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CAGA(nn.Module):\n",
    "    \"\"\"\n",
    "      Attributes:\n",
    "          heads: Number of attention heads in the multi-head attention mechanism.\n",
    "          dim: Dimensionality of Q, K, V.\n",
    "          scale: Scaling factor for the attention computation.\n",
    "          head_dim: The number of channels per attention head.\n",
    "          dilation: List of dilation values for dilated convolutions.\n",
    "          total_layer: Total number of layers used in the module.\n",
    "          get_begin: Initial convolutional layer for feature extraction.\n",
    "          get_qkv: List of convolutional layers to compute queries, keys, and values.\n",
    "          convert_to_headdim: Layer to combine and reshape the outputs of all dilated convolutions.\n",
    "          mix: Convolutional layers to refine the features.\n",
    "          proj: Final projection layer to map the concatenated features to the input shape.\n",
    "          norm: Batch normalization layer to normalize the output.\n",
    "      \"\"\"\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            activation,\n",
    "            heads = 3,\n",
    "            dim = 8,\n",
    "            expand_ratio = 4,\n",
    "            head_dim = 16,\n",
    "            dilation = (1,2),\n",
    "            random_seed = 82\n",
    "            ):\n",
    "\n",
    "        # Set global random seeds for reproducibility\n",
    "        self._set_global_seeds(random_seed)\n",
    "\n",
    "        super(CAGA, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.dim = dim\n",
    "        scale = dim\n",
    "        self.scale = dim ** -0.5\n",
    "        self.head_dim = head_dim\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.total_layer = 4\n",
    "\n",
    "        # Reproducible layer initialization\n",
    "        self.get_begin = self._init_depthwise_separable_conv(\n",
    "            DepthWiseSeperableConvLayer(in_channels, self.heads * self.head_dim)\n",
    "        )\n",
    "\n",
    "        self.get_qkv = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                self._init_conv(nn.Conv2d(\n",
    "                    self.head_dim,\n",
    "                    self.head_dim,\n",
    "                    3,\n",
    "                    groups=self.head_dim,\n",
    "                    dilation=di\n",
    "                )),\n",
    "                self._init_conv(nn.Conv2d(self.head_dim, 3 * self.dim, 1, groups=1))\n",
    "            )\n",
    "            for di in dilation\n",
    "        ])\n",
    "\n",
    "        self.convert_to_headdim = self._init_conv(\n",
    "            nn.Conv2d(len(dilation) * self.dim, self.head_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.mix = nn.Sequential(\n",
    "            self._init_conv(nn.Conv2d(\n",
    "                self.dim,\n",
    "                self.dim * 3,\n",
    "                1\n",
    "            )),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.proj = self._init_conv(\n",
    "            nn.Conv2d(self.heads * self.dim * len(self.dilation), in_channels, 1)\n",
    "        )\n",
    "\n",
    "        # Deterministic BatchNorm\n",
    "        self.norm = nn.BatchNorm2d(num_features=in_channels, affine=True)\n",
    "        nn.init.constant_(self.norm.weight, 1)\n",
    "        nn.init.constant_(self.norm.bias, 0)\n",
    "\n",
    "    def _set_global_seeds(self, seed):\n",
    "        \"\"\"Set seeds for reproducibility across libraries.\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    def _init_conv(self, conv_layer):\n",
    "        \"\"\"Initialize convolutional layer weights deterministically.\"\"\"\n",
    "        nn.init.xavier_uniform_(conv_layer.weight)\n",
    "        if conv_layer.bias is not None:\n",
    "            nn.init.constant_(conv_layer.bias, 0)\n",
    "        return conv_layer\n",
    "\n",
    "    def _init_depthwise_separable_conv(self, conv_layer):\n",
    "        \"\"\"Initialize depthwise separable convolution layers.\"\"\"\n",
    "        # Assuming DepthWiseSeperableConvLayer has similar structure to standard conv\n",
    "        for m in conv_layer.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        return conv_layer\n",
    "        # self.norm = nn.LayerNorm([8, 256, 14, 14])\n",
    "\n",
    "\n",
    "\n",
    "    def attention(self,q,k,v , shape):\n",
    "\n",
    "        B, C, H, W = shape\n",
    "\n",
    "\n",
    "\n",
    "        q, k, v = q.float(), k.float(), v.float()\n",
    "\n",
    "\n",
    "        q = q * self.scale\n",
    "        att_map = q.transpose(-2, -1) @ k\n",
    "\n",
    "\n",
    "        att_map = att_map.softmax(dim=-1)\n",
    "\n",
    "\n",
    "        out = v @ att_map.transpose(-2, -1)\n",
    "\n",
    "        out = out.view(B , -1 , H , W)\n",
    "\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self,x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "\n",
    "\n",
    "            # print(op)\n",
    "\n",
    "        x_copy = x\n",
    "\n",
    "        all_heads  = self.get_begin(x)\n",
    "\n",
    "        multi_layer = all_heads.split([self.head_dim]*self.heads , dim=1)\n",
    "\n",
    "        all_heads_after_op = [[]]*self.heads\n",
    "\n",
    "\n",
    "\n",
    "        for j in range(self.heads):\n",
    "\n",
    "            for op in self.get_qkv:\n",
    "\n",
    "                all_heads_after_op[j].append(op(multi_layer[j]))\n",
    "\n",
    "\n",
    "        all_final = []\n",
    "\n",
    "\n",
    "        for i in range(self.heads):\n",
    "            out_all = []\n",
    "            for j in range(len(self.dilation ) ):\n",
    "\n",
    "\n",
    "\n",
    "                q , k , v = all_heads_after_op[i][j].split([self.dim, self.dim, self.dim], dim=1)\n",
    "                shape = q.shape\n",
    "                q, k, v = q.flatten(2), k.flatten(2), v.flatten(2)\n",
    "                out = self.attention(q , k , v , shape)\n",
    "\n",
    "\n",
    "\n",
    "                # print(out.shape)\n",
    "                shape_ahead = all_heads_after_op[i][j+1].shape[3]\n",
    "\n",
    "                temp = torchvision.transforms.functional.resize(out , (shape_ahead,shape_ahead))\n",
    "\n",
    "                out = F.interpolate(out, size=( H , W ), mode='bilinear')\n",
    "                out = out.view(B, self.dim, H, W)\n",
    "\n",
    "\n",
    "                if j+1 != len(self.dilation ):\n",
    "\n",
    "                    temp = self.mix(temp).clone()\n",
    "                    all_heads_after_op[i][j+1] = all_heads_after_op[i][j+1] + temp\n",
    "\n",
    "                out_all.append(out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            out_all_one = torch.cat(out_all, dim=1)\n",
    "            all_final.append(out_all_one)\n",
    "            if i+1 != self.heads:\n",
    "                all_heads_after_op[i+1] += self.convert_to_headdim(out_all_one)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      # we need to billinear intterpolate before append\n",
    "        all_concat = torch.cat(all_final, dim=1)\n",
    "\n",
    "\n",
    "        x_final = self.proj(all_concat) + x_copy # try oncat later\n",
    "\n",
    "        return self.norm (x_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fef97e6c-20f5-47bf-8a91-6ec50aea4c48",
   "metadata": {
    "id": "fef97e6c-20f5-47bf-8a91-6ec50aea4c48"
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.max_validation_acc = float('-inf')\n",
    "\n",
    "    def early_stop(self, validation_acc):\n",
    "\n",
    "        if validation_acc > self.max_validation_acc:\n",
    "            self.max_validation_acc = validation_acc\n",
    "            self.counter = 0\n",
    "        elif validation_acc <= (self.max_validation_acc - self.min_delta):\n",
    "            print(\"max_acc\" , self.max_validation_acc)\n",
    "\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return (True,self.counter)\n",
    "        return (False,self.counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "-Eqwbs0Qv3PQ",
   "metadata": {
    "id": "-Eqwbs0Qv3PQ"
   },
   "outputs": [],
   "source": [
    "class FocalCELoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', num_classes=4, device=None):\n",
    "        \"\"\"\n",
    "        Focal Cross Entropy Loss\n",
    "\n",
    "        Args:\n",
    "            alpha (torch.Tensor, optional): Weight for each class. Must be of size C.\n",
    "            gamma (float): Focusing parameter to reduce the relative loss for well-classified examples.\n",
    "            reduction (str): Reduction method for the final loss ('none', 'mean', or 'sum').\n",
    "            num_classes (int): Number of classes in the classification task.\n",
    "            device (torch.device): Device on which to place the class weights and other tensors.\n",
    "        \"\"\"\n",
    "        super(FocalCELoss, self).__init__()\n",
    "        self.gamma = gamma  # Focusing parameter for down-weighting well-classified examples\n",
    "        self.reduction = reduction  # Reduction method for the loss\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Handle class weights (alpha) if provided\n",
    "        if alpha is not None:\n",
    "            if isinstance(alpha, (list, tuple)):\n",
    "                self.alpha = torch.tensor(alpha)  # Convert list/tuple to tensor\n",
    "            else:\n",
    "                self.alpha = alpha  # Use alpha directly if it is already a tensor\n",
    "            assert len(self.alpha) == num_classes, \"Alpha size must match the number of classes\"\n",
    "            # Move alpha tensor to the specified device\n",
    "            self.alpha = self.alpha.to(self.device)\n",
    "        else:\n",
    "            self.alpha = None  # If alpha is not provided, no weighting is applied\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass to compute the Focal Cross Entropy Loss.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (N, C), where N is the batch size and C is the number of classes.\n",
    "            targets: Tensor of shape (N,), containing class indices in the range [0, C-1].\n",
    "        \"\"\"\n",
    "        # Ensure inputs and targets are on the same device as the class weights (alpha)\n",
    "        inputs = inputs.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "\n",
    "        # Compute the standard cross-entropy loss without reduction\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
    "\n",
    "        # Compute the probability of the predicted class (pt)\n",
    "        pt = torch.exp(-ce_loss)  # pt is the probability of the true class\n",
    "\n",
    "        # Compute the Focal Loss by scaling the cross-entropy loss with the modulating factor\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        # Apply the specified reduction method\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)  # Return the mean loss\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)  # Return the sum of the loss\n",
    "        else:  # 'none'\n",
    "            return focal_loss  # Return the loss for each sample without reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "DaODGtXuzyVD",
   "metadata": {
    "id": "DaODGtXuzyVD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "HYPERPARAMS = {\n",
    "    'EPOCH': 15,\n",
    "    'dropout': 0.111975,\n",
    "    'neurons1': 1024,\n",
    "    'neurons2': 2048,\n",
    "    'neurons3': 512,\n",
    "    'neurons4': 512,\n",
    "    'n_layers': 2,\n",
    "    'batch_size_train': 8,\n",
    "\n",
    "    'batch_size_test': 8,\n",
    "    'learning_rate': 0.0000603447274,\n",
    "    'weight_decay': 0.01,\n",
    "\n",
    "    'min_delta': 0,\n",
    "    'scheduler_gamma': 0.95\n",
    "}\n",
    "seed = 45\n",
    "def training(model_name , model_config_name, HYPERPARAMS,  X_train, y_train,  X_test, y_test, use_CAGA = False , pre_trained = True, run = 1 ):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Create necessary directories\n",
    "    os.makedirs(f'./save/mymodules/run_{run}', exist_ok=True)\n",
    "    os.makedirs(f'./save_data/run_{run}', exist_ok=True)\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    accuracy_dict_train = []\n",
    "    loss_dict_train = []\n",
    "    accuracy_dict_val = []\n",
    "    loss_dict_val = []\n",
    "    test_metrics_history = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    X_train = X_train.float()\n",
    "    y_train = y_train.float()\n",
    "\n",
    "    model_name = model_name\n",
    "\n",
    "    csv_dir = f'/results/run_{run}'\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "    main_csv_path = os.path.join(csv_dir, f'results_{timestamp}_{model_name}.csv')\n",
    "\n",
    "    # Initialize main results CSV\n",
    "    header = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1'] + \\\n",
    "         [f'{class_name} Accuracy' for class_name in class_labels.keys()] + \\\n",
    "         list(HYPERPARAMS.keys())\n",
    "\n",
    "    with open(main_csv_path, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(header)\n",
    "    #     Monkeypox: 0\n",
    "# Chickenpox: 1\n",
    "# Measles: 2\n",
    "# Normal: 3\n",
    "\n",
    "    # Create model\n",
    "    X_train, X_test = norm(X_train,X_test)\n",
    "    model = timm.create_model(model_config_name, pretrained= pre_trained) #for ablation turn pre-trained to False\n",
    "    if use_CAGA:\n",
    "        replace_context_modules(model, CAGA, dilation = HYPERPARAMS['dilation']) # comment this for all other models apart for efficientViT-CAGA\n",
    "    change_classifier(model, model_config_name, dropout=HYPERPARAMS['dropout'], neurons1=HYPERPARAMS['neurons1'], neurons2=HYPERPARAMS['neurons2'], neurons3=HYPERPARAMS['neurons3'], neurons4=HYPERPARAMS['neurons4'], n_layers=HYPERPARAMS['n_layers'])\n",
    "\n",
    "    print(model)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    # Create data loaders\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    training_loader = DataLoader(\n",
    "        list(zip(X_train.to(device), y_train.to(device))),\n",
    "        batch_size=HYPERPARAMS['batch_size_train'],\n",
    "        shuffle=True,\n",
    "        generator=generator,\n",
    "        worker_init_fn=lambda worker_id: torch.manual_seed(seed + worker_id)\n",
    "    )\n",
    "\n",
    "    # Test loader doesn't need shuffling but we'll set the seed for consistency\n",
    "    test_loader = DataLoader(\n",
    "        list(zip(X_test.to(device), y_test.to(device))),\n",
    "        batch_size=HYPERPARAMS['batch_size_test'],\n",
    "        shuffle=False,\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "    #alpha = torch.tensor([1.25, 0.61, 0.81, 0.59]).to(device)\n",
    "    alpha = torch.tensor([1.29, 0.66, 1.15, 0.81]).to(device)\n",
    "    #alpha = torch.tensor([1.25 ,1.35,1.15, 0.51]).to(device)\n",
    "    # Loss functions and optimizer setup\n",
    "    #loss_fn = FocalCELoss( alpha = alpha, gamma=2.0, num_classes=4)\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    loss_fn1 = torch.nn.CrossEntropyLoss(weight = alpha)\n",
    "    w = 0\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=HYPERPARAMS['learning_rate'], weight_decay=HYPERPARAMS['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=HYPERPARAMS['scheduler_gamma'])\n",
    "    #early_stopper = EarlyStopper(patience=HYPERPARAMS['patience'], min_delta=HYPERPARAMS['min_delta'])\n",
    "\n",
    "    def evaluate_on_test():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc, precision, recall, f1, *class_accuracies_test = test(model, test_loader, 0)\n",
    "            print('\\n##########################################################')\n",
    "            print(f'Intermediate Test Results:')\n",
    "            print(f'Accuracy test: {acc:.4f}')\n",
    "            print(f'Precision test: {precision:.4f}')\n",
    "            print(f'Recall test: {recall:.4f}')\n",
    "            print(f'F1 test: {f1:.4f}')\n",
    "            print('---------->classwise test<-----------')\n",
    "            for i, class_name in enumerate(class_labels.keys()):\n",
    "                print(f'Accuracy {class_name} test: {class_accuracies_test[i]:.4f}')\n",
    "            print('##########################################################\\n')\n",
    "\n",
    "            return {\n",
    "                'epoch': epoch,  # Corrected indentation here\n",
    "                'accuracy': acc,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'class_accuracies': class_accuracies_test\n",
    "            }\n",
    "\n",
    "    # Rest of the function remains the same...\n",
    "    # Training loop\n",
    "    for epoch in range(HYPERPARAMS['EPOCH']):\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        print(f'EPOCH {epoch }:')\n",
    "\n",
    "        # Training phase\n",
    "        model.train(True)\n",
    "        avg_loss_train, train_accuracy, right, *class_accuracies_train = train_one_epoch(\n",
    "            model, epoch, 0, training_loader, loss_fn, loss_fn1, w, optimizer, loss_dict_train\n",
    "        )\n",
    "        accuracy_dict_train.append(train_accuracy)\n",
    "        loss_dict_train.append(avg_loss_train)\n",
    "\n",
    "        print(f\"Average training loss: {avg_loss_train}\")\n",
    "        print(f'Right train {right}')\n",
    "        print(f'Accuracy train {train_accuracy}')\n",
    "\n",
    "        print('---------->classwise train<-----------')\n",
    "        for i, class_name in enumerate(class_labels.keys()):\n",
    "            print(f'Accuracy {class_name} train {class_accuracies_train[i]}')\n",
    "\n",
    "        # Evaluate on test set every 1 epoch\n",
    "        #if (epoch + 1) % 1 == 0:\n",
    "         #   test_metrics = evaluate_on_test()\n",
    "          #  test_metrics_history.append(test_metrics)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Final evaluation\n",
    "    final_test_metrics = evaluate_on_test()\n",
    "    test_metrics_history.append(final_test_metrics)\n",
    "\n",
    "    row = [model_name, final_test_metrics['accuracy'], final_test_metrics['precision'], final_test_metrics['recall'], final_test_metrics['f1']] + \\\n",
    "                    final_test_metrics['class_accuracies'] + \\\n",
    "                    list(HYPERPARAMS.values())\n",
    "\n",
    "    with open(main_csv_path, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "    return {\n",
    "        'accuracy': final_test_metrics['accuracy'],\n",
    "        'precision': final_test_metrics['precision'],\n",
    "        'recall': final_test_metrics['recall'],\n",
    "        'f1': final_test_metrics['f1'],\n",
    "        'train_accuracies': accuracy_dict_train,\n",
    "        'train_losses': loss_dict_train,\n",
    "        'class_accuracies': final_test_metrics['class_accuracies'],\n",
    "        'test_history': test_metrics_history\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28037a2c-5fb4-47df-b0fa-4b423274abf1",
   "metadata": {
    "id": "28037a2c-5fb4-47df-b0fa-4b423274abf1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
